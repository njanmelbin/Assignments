{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhk_COC8we51"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-22 07:30:42--  https://doc-04-8s-docs.googleusercontent.com/docs/securesc/6qasdsvtcepj72nj3126fhrlbctlsaju/eefs3cvrbm4u1ju1gt9mistifrov2g2e/1548136800000/06629147635963609455/12370343985631133257/118toouH8ifByKJHmmowyjOjGyir-YAvi?e=download&nonce=8c5pco6po95mg&user=12370343985631133257&hash=ue43mlk075cdbk64ucdojcp3urk3ujmj\n",
      "Resolving doc-04-8s-docs.googleusercontent.com (doc-04-8s-docs.googleusercontent.com)... 74.125.141.132, 2607:f8b0:400c:c06::84\n",
      "Connecting to doc-04-8s-docs.googleusercontent.com (doc-04-8s-docs.googleusercontent.com)|74.125.141.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘HumanActivityRecognition.zip’\n",
      "\n",
      "HumanActivityRecogn     [     <=>            ]  85.62M  58.7MB/s    in 1.5s    \n",
      "\n",
      "2019-01-22 07:30:44 (58.7 MB/s) - ‘HumanActivityRecognition.zip’ saved [89781419]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --header=\"Host: doc-04-8s-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" --header=\"Referer: https://drive.google.com/\" --header=\"Cookie: AUTH_ucu55st84n9vmnrguf06csv12kkku9tn_nonce=8c5pco6po95mg\" --header=\"Connection: keep-alive\" \"https://doc-04-8s-docs.googleusercontent.com/docs/securesc/6qasdsvtcepj72nj3126fhrlbctlsaju/eefs3cvrbm4u1ju1gt9mistifrov2g2e/1548136800000/06629147635963609455/12370343985631133257/118toouH8ifByKJHmmowyjOjGyir-YAvi?e=download&nonce=8c5pco6po95mg&user=12370343985631133257&hash=ue43mlk075cdbk64ucdojcp3urk3ujmj\" -O \"HumanActivityRecognition.zip\" -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  HumanActivityRecognition.zip\n",
      "   creating: HAR/\n",
      "  inflating: HAR/.DS_Store           \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/HAR/\n",
      "  inflating: __MACOSX/HAR/._.DS_Store  \n",
      "   creating: HAR/.ipynb_checkpoints/\n",
      "  inflating: HAR/.ipynb_checkpoints/HAR_EDA-checkpoint.ipynb  \n",
      "  inflating: HAR/.ipynb_checkpoints/HAR_LSTM-checkpoint.ipynb  \n",
      "  inflating: HAR/.ipynb_checkpoints/HAR_LSTM_1-checkpoint.ipynb  \n",
      "  inflating: HAR/.ipynb_checkpoints/HAR_PREDICTION_MODELS-checkpoint.ipynb  \n",
      "  inflating: HAR/HAR_EDA.ipynb       \n",
      "  inflating: __MACOSX/HAR/._HAR_EDA.ipynb  \n",
      "  inflating: HAR/HAR_LSTM.ipynb      \n",
      "  inflating: __MACOSX/HAR/._HAR_LSTM.ipynb  \n",
      "  inflating: HAR/HAR_PREDICTION_MODELS.ipynb  \n",
      "  inflating: __MACOSX/HAR/._HAR_PREDICTION_MODELS.ipynb  \n",
      "  inflating: HAR/t-sne_perp_10_iter_1000.png  \n",
      "  inflating: __MACOSX/HAR/._t-sne_perp_10_iter_1000.png  \n",
      "  inflating: HAR/t-sne_perp_20_iter_1000.png  \n",
      "  inflating: __MACOSX/HAR/._t-sne_perp_20_iter_1000.png  \n",
      "  inflating: HAR/t-sne_perp_2_iter_1000.png  \n",
      "  inflating: __MACOSX/HAR/._t-sne_perp_2_iter_1000.png  \n",
      "  inflating: HAR/t-sne_perp_50_iter_1000.png  \n",
      "  inflating: __MACOSX/HAR/._t-sne_perp_50_iter_1000.png  \n",
      "  inflating: HAR/t-sne_perp_5_iter_1000.png  \n",
      "  inflating: __MACOSX/HAR/._t-sne_perp_5_iter_1000.png  \n",
      "   creating: HAR/UCI_HAR_Dataset/\n",
      "  inflating: HAR/UCI_HAR_Dataset/.DS_Store  \n",
      "   creating: __MACOSX/HAR/UCI_HAR_Dataset/\n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/._.DS_Store  \n",
      "  inflating: HAR/UCI_HAR_Dataset/_DS_Store  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/.__DS_Store  \n",
      "  inflating: HAR/UCI_HAR_Dataset/activity_labels.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/._activity_labels.txt  \n",
      "   creating: HAR/UCI_HAR_Dataset/csv_files/\n",
      "  inflating: HAR/UCI_HAR_Dataset/csv_files/test.csv  \n",
      "   creating: __MACOSX/HAR/UCI_HAR_Dataset/csv_files/\n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/csv_files/._test.csv  \n",
      "  inflating: HAR/UCI_HAR_Dataset/csv_files/train.csv  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/csv_files/._train.csv  \n",
      "  inflating: HAR/UCI_HAR_Dataset/features.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/._features.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/features_info.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/._features_info.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/README.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/._README.txt  \n",
      "   creating: HAR/UCI_HAR_Dataset/test/\n",
      "   creating: HAR/UCI_HAR_Dataset/test/Inertial Signals/\n",
      "  inflating: HAR/UCI_HAR_Dataset/test/Inertial Signals/body_acc_x_test.txt  \n",
      "   creating: __MACOSX/HAR/UCI_HAR_Dataset/test/\n",
      "   creating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/\n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/._body_acc_x_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/Inertial Signals/body_acc_y_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/._body_acc_y_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/Inertial Signals/body_acc_z_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/._body_acc_z_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/Inertial Signals/body_gyro_x_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/._body_gyro_x_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/Inertial Signals/body_gyro_y_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/._body_gyro_y_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/Inertial Signals/body_gyro_z_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/._body_gyro_z_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/Inertial Signals/total_acc_x_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/._total_acc_x_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/Inertial Signals/total_acc_y_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/._total_acc_y_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/Inertial Signals/total_acc_z_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/Inertial Signals/._total_acc_z_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/subject_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/._subject_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/X_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/._X_test.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/test/y_test.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/test/._y_test.txt  \n",
      "   creating: HAR/UCI_HAR_Dataset/train/\n",
      "  inflating: HAR/UCI_HAR_Dataset/train/.DS_Store  \n",
      "   creating: __MACOSX/HAR/UCI_HAR_Dataset/train/\n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/._.DS_Store  \n",
      "   creating: HAR/UCI_HAR_Dataset/train/Inertial Signals/\n",
      "  inflating: HAR/UCI_HAR_Dataset/train/Inertial Signals/body_acc_x_train.txt  \n",
      "   creating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/\n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/._body_acc_x_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/Inertial Signals/body_acc_y_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/._body_acc_y_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/Inertial Signals/body_acc_z_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/._body_acc_z_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/Inertial Signals/body_gyro_x_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/._body_gyro_x_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/Inertial Signals/body_gyro_y_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/._body_gyro_y_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/Inertial Signals/body_gyro_z_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/._body_gyro_z_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/Inertial Signals/total_acc_x_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/._total_acc_x_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/Inertial Signals/total_acc_y_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/._total_acc_y_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/Inertial Signals/total_acc_z_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/Inertial Signals/._total_acc_z_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/subject_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/._subject_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/X_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/._X_train.txt  \n",
      "  inflating: HAR/UCI_HAR_Dataset/train/y_train.txt  \n",
      "  inflating: __MACOSX/HAR/UCI_HAR_Dataset/train/._y_train.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip HumanActivityRecognition.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperas in ./.local/lib/python3.5/site-packages\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.5/dist-packages (from hyperas)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.5/dist-packages (from hyperas)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.5/dist-packages (from hyperas)\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.5/dist-packages (from hyperas)\n",
      "Requirement already satisfied: hyperopt in ./.local/lib/python3.5/site-packages (from hyperas)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.5/dist-packages (from hyperas)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras->hyperas)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from keras->hyperas)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras->hyperas)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from keras->hyperas)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras->hyperas)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/site-packages (from keras->hyperas)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/site-packages (from keras->hyperas)\n",
      "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.5/dist-packages (from nbformat->hyperas)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.5/dist-packages (from nbformat->hyperas)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.5/dist-packages (from nbformat->hyperas)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.5/dist-packages (from nbformat->hyperas)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.5/dist-packages (from jupyter->hyperas)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.5/dist-packages (from jupyter->hyperas)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.5/dist-packages (from jupyter->hyperas)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.5/dist-packages (from jupyter->hyperas)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.5/dist-packages (from jupyter->hyperas)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.5/dist-packages (from hyperopt->hyperas)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.5/dist-packages (from hyperopt->hyperas)\n",
      "Requirement already satisfied: pymongo in ./.local/lib/python3.5/site-packages (from hyperopt->hyperas)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.5/dist-packages (from nbconvert->hyperas)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.5/dist-packages (from nbconvert->hyperas)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->hyperas)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.5/dist-packages (from nbconvert->hyperas)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.5/dist-packages (from nbconvert->hyperas)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.5/dist-packages (from nbconvert->hyperas)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->hyperas)\n",
      "Requirement already satisfied: intel-numpy in /usr/local/lib/python3.5/dist-packages (from scipy>=0.14->keras->hyperas)\n",
      "Requirement already satisfied: icc-rt in /usr/local/lib/python3.5/dist-packages (from numpy>=1.9.1->keras->hyperas)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.5/dist-packages (from numpy>=1.9.1->keras->hyperas)\n",
      "Requirement already satisfied: mkl-random in /usr/local/lib/python3.5/dist-packages (from numpy>=1.9.1->keras->hyperas)\n",
      "Requirement already satisfied: mkl-fft in /usr/local/lib/python3.5/dist-packages (from numpy>=1.9.1->keras->hyperas)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.5/dist-packages (from numpy>=1.9.1->keras->hyperas)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.1->nbformat->hyperas)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.5/dist-packages (from jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.5/dist-packages (from jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.5/dist-packages (from ipykernel->jupyter->hyperas)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter->hyperas)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter->hyperas)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter->hyperas)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter->hyperas)\n",
      "Requirement already satisfied: widgetsnbextension~=3.3.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets->jupyter->hyperas)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.5/dist-packages (from jinja2->nbconvert->hyperas)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.5/dist-packages (from bleach->nbconvert->hyperas)\n",
      "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.5/dist-packages (from icc-rt->numpy>=1.9.1->keras->hyperas)\n",
      "Requirement already satisfied: tbb==2019.* in /usr/local/lib/python3.5/dist-packages (from tbb4py->numpy>=1.9.1->keras->hyperas)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.5/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.5/dist-packages (from ipython->jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.5/dist-packages (from ipython->jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.5/dist-packages (from ipython->jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.5/dist-packages (from ipython->jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.5/dist-packages (from ipython->jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.5/dist-packages (from terminado>=0.8.1->notebook->jupyter->hyperas)\n",
      "Requirement already satisfied: parso>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from jedi>=0.10->ipython->jupyter-console->jupyter->hyperas)\n",
      "Requirement already satisfied: hyperopt in ./.local/lib/python3.5/site-packages\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/site-packages (from hyperopt)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.5/dist-packages (from hyperopt)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from hyperopt)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.5/site-packages (from hyperopt)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.5/dist-packages (from hyperopt)\n",
      "Requirement already satisfied: pymongo in ./.local/lib/python3.5/site-packages (from hyperopt)\n",
      "Requirement already satisfied: mkl-random in /usr/local/lib/python3.5/dist-packages (from numpy->hyperopt)\n",
      "Requirement already satisfied: mkl-fft in /usr/local/lib/python3.5/dist-packages (from numpy->hyperopt)\n",
      "Requirement already satisfied: icc-rt in /usr/local/lib/python3.5/dist-packages (from numpy->hyperopt)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.5/dist-packages (from numpy->hyperopt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.5/dist-packages (from numpy->hyperopt)\r\n",
      "Requirement already satisfied: intel-numpy in /usr/local/lib/python3.5/dist-packages (from scipy->hyperopt)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.5/dist-packages (from networkx->hyperopt)\r\n",
      "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.5/dist-packages (from icc-rt->numpy->hyperopt)\r\n",
      "Requirement already satisfied: tbb==2019.* in /usr/local/lib/python3.5/dist-packages (from tbb4py->numpy->hyperopt)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install hyperas\n",
    "!pip3 install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kR6sUYVImiTD"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMwNQZg1we57"
   },
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wfq0K_Kwe5-"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HWJGoABwe5_"
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KLV7GhZwe6P"
   },
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1pG8Ehpwe6V"
   },
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uNFhorbQwe6Y",
    "outputId": "d7e4d822-51c2-44ad-a6ab-677a1c054b20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F49BNpgnwe6b"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    import numpy as np\n",
    "    \n",
    "    # Raw data signals\n",
    "    # Signals are from Accelerometer and Gyroscope\n",
    "    # The signals are in x,y,z directions\n",
    "    # Sensor signals are filtered to have only body acceleration\n",
    "    # excluding the acceleration due to gravity\n",
    "    # Triaxial acceleration from the accelerometer is total acceleration\n",
    "    SIGNALS = [\n",
    "        \"body_acc_x\",\n",
    "        \"body_acc_y\",\n",
    "        \"body_acc_z\",\n",
    "        \"body_gyro_x\",\n",
    "        \"body_gyro_y\",\n",
    "        \"body_gyro_z\",\n",
    "        \"total_acc_x\",\n",
    "        \"total_acc_y\",\n",
    "        \"total_acc_z\"\n",
    "    ]\n",
    "    # Utility function to load the load\n",
    "    # Utility function to read the data from csv file\n",
    "    def _read_csv(filename):\n",
    "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "      \n",
    "    def load_y(subset):\n",
    "        \"\"\"\n",
    "        The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "        that represents a human activity. We return a binary representation of \n",
    "        every sample objective as a 6 bits vector using One Hot Encoding\n",
    "        (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "        \"\"\"\n",
    "        filename = 'HAR/UCI_HAR_Dataset/' + subset+ '/y_'+ subset+'.txt'\n",
    "        y = _read_csv(filename)[0]\n",
    "\n",
    "        return pd.get_dummies(y).as_matrix()\n",
    "    def load_signals(subset):\n",
    "        signals_data = []\n",
    "\n",
    "        for signal in SIGNALS:\n",
    "            filename = 'HAR/UCI_HAR_Dataset/'+subset+'/Inertial Signals/'+signal+'_'+subset+'.txt'\n",
    "            signals_data.append(\n",
    "                _read_csv(filename).as_matrix()\n",
    "            ) \n",
    "\n",
    "          # Transpose is used to change the dimensionality of the output,\n",
    "          # aggregating the signals by combination of sample/timestep.\n",
    "          # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "        return np.transpose(signals_data, (1, 2, 0))\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    Y_train, Y_test = load_y('train'), load_y('test')\n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3X3KBQH7vAJ"
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test):\n",
    "    model = Sequential()\n",
    "    # Configuring the parameters\n",
    "    model.add(LSTM({{choice([64, 128, 256])}}, input_shape=(128, 9)))\n",
    "    # Adding a dropout layer\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    # Adding a dense output layer with sigmoid activation\n",
    "    model.add(Dense(6, activation='sigmoid'))\n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    history = model.fit(X_train,\n",
    "            Y_train,\n",
    "            batch_size={{choice([16,32,64])}},\n",
    "            validation_data=(X_test, Y_test),\n",
    "            epochs=30)\n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model,'history.val_loss':history.history['val_loss'], 'history.val_acc': history.history['val_acc'],\n",
    "    'history.loss': history.history['loss'], 'history.acc': history.history['acc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2ImZP73rtGR"
   },
   "outputs": [],
   "source": [
    "from hyperas import optim\n",
    "from hyperopt import tpe,Trials,STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "JV6Vw9J1Bxly",
    "outputId": "80c8d682-ece9-4dad-b9ed-2df546def614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import warnings\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import tpe, Trials, STATUS_OK\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from collections import defaultdict\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'LSTM': hp.choice('LSTM', [64, 128, 256]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'batch_size': hp.choice('batch_size', [16,32,64]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: import numpy as np\n",
      "   3: \n",
      "   4: # Raw data signals\n",
      "   5: # Signals are from Accelerometer and Gyroscope\n",
      "   6: # The signals are in x,y,z directions\n",
      "   7: # Sensor signals are filtered to have only body acceleration\n",
      "   8: # excluding the acceleration due to gravity\n",
      "   9: # Triaxial acceleration from the accelerometer is total acceleration\n",
      "  10: SIGNALS = [\n",
      "  11:     \"body_acc_x\",\n",
      "  12:     \"body_acc_y\",\n",
      "  13:     \"body_acc_z\",\n",
      "  14:     \"body_gyro_x\",\n",
      "  15:     \"body_gyro_y\",\n",
      "  16:     \"body_gyro_z\",\n",
      "  17:     \"total_acc_x\",\n",
      "  18:     \"total_acc_y\",\n",
      "  19:     \"total_acc_z\"\n",
      "  20: ]\n",
      "  21: # Utility function to load the load\n",
      "  22: # Utility function to read the data from csv file\n",
      "  23: def _read_csv(filename):\n",
      "  24:     return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
      "  25:   \n",
      "  26: def load_y(subset):\n",
      "  27:     \"\"\"\n",
      "  28:     The objective that we are trying to predict is a integer, from 1 to 6,\n",
      "  29:     that represents a human activity. We return a binary representation of \n",
      "  30:     every sample objective as a 6 bits vector using One Hot Encoding\n",
      "  31:     (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
      "  32:     \"\"\"\n",
      "  33:     filename = 'HAR/UCI_HAR_Dataset/' + subset+ '/y_'+ subset+'.txt'\n",
      "  34:     y = _read_csv(filename)[0]\n",
      "  35: \n",
      "  36:     return pd.get_dummies(y).as_matrix()\n",
      "  37: def load_signals(subset):\n",
      "  38:     signals_data = []\n",
      "  39: \n",
      "  40:     for signal in SIGNALS:\n",
      "  41:         filename = 'HAR/UCI_HAR_Dataset/'+subset+'/Inertial Signals/'+signal+'_'+subset+'.txt'\n",
      "  42:         signals_data.append(\n",
      "  43:             _read_csv(filename).as_matrix()\n",
      "  44:         ) \n",
      "  45: \n",
      "  46:       # Transpose is used to change the dimensionality of the output,\n",
      "  47:       # aggregating the signals by combination of sample/timestep.\n",
      "  48:       # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
      "  49:     return np.transpose(signals_data, (1, 2, 0))\n",
      "  50: \"\"\"\n",
      "  51: Obtain the dataset from multiple files.\n",
      "  52: Returns: X_train, X_test, y_train, y_test\n",
      "  53: \"\"\"\n",
      "  54: X_train, X_test = load_signals('train'), load_signals('test')\n",
      "  55: Y_train, Y_test = load_y('train'), load_y('test')\n",
      "  56: \n",
      "  57: \n",
      "  58: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     model = Sequential()\n",
      "  4:     # Configuring the parameters\n",
      "  5:     model.add(LSTM(space['LSTM'], input_shape=(128, 9)))\n",
      "  6:     # Adding a dropout layer\n",
      "  7:     model.add(Dropout(space['Dropout']))\n",
      "  8:     # Adding a dense output layer with sigmoid activation\n",
      "  9:     model.add(Dense(6, activation='sigmoid'))\n",
      " 10:     # Compiling the model\n",
      " 11:     model.compile(loss='categorical_crossentropy',\n",
      " 12:               optimizer='rmsprop',\n",
      " 13:               metrics=['accuracy'])\n",
      " 14:     # Training the model\n",
      " 15:     history = model.fit(X_train,\n",
      " 16:             Y_train,\n",
      " 17:             batch_size=space['batch_size'],\n",
      " 18:             validation_data=(X_test, Y_test),\n",
      " 19:             epochs=30)\n",
      " 20:     score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
      " 21:     print('Test accuracy:', acc)\n",
      " 22:     return {'loss': -acc, 'status': STATUS_OK, 'model': model,'history.val_loss':history.history['val_loss'], 'history.val_acc': history.history['val_acc'],\n",
      " 23:     'history.loss': history.history['loss'], 'history.acc': history.history['acc']\n",
      " 24:     }\n",
      " 25: \n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 1.4209 - acc: 0.3564 - val_loss: 1.4125 - val_acc: 0.4544\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 1.1910 - acc: 0.4708 - val_loss: 1.0844 - val_acc: 0.5324\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.9676 - acc: 0.5657 - val_loss: 1.0290 - val_acc: 0.5019\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 1.0702 - acc: 0.5178 - val_loss: 1.0130 - val_acc: 0.5151\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8446 - acc: 0.6079 - val_loss: 1.0241 - val_acc: 0.5623\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.7872 - acc: 0.6158 - val_loss: 0.9752 - val_acc: 0.5626\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.7344 - acc: 0.6364 - val_loss: 0.7688 - val_acc: 0.6318\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.6956 - acc: 0.6529 - val_loss: 0.8160 - val_acc: 0.6128\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.7003 - acc: 0.6548 - val_loss: 0.7380 - val_acc: 0.6108\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.6681 - acc: 0.6625 - val_loss: 0.7031 - val_acc: 0.6244\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.6966 - acc: 0.6591 - val_loss: 0.7415 - val_acc: 0.6186\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.6698 - acc: 0.6692 - val_loss: 0.6666 - val_acc: 0.6301\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.6118 - acc: 0.6892 - val_loss: 0.7424 - val_acc: 0.6471\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.6177 - acc: 0.7038 - val_loss: 0.6259 - val_acc: 0.7299\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.5082 - acc: 0.7886 - val_loss: 0.7070 - val_acc: 0.7109\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.4196 - acc: 0.8640 - val_loss: 0.4676 - val_acc: 0.8473\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.3333 - acc: 0.8909 - val_loss: 0.3985 - val_acc: 0.8819\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.2580 - acc: 0.9183 - val_loss: 0.4600 - val_acc: 0.8466\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.2347 - acc: 0.9207 - val_loss: 0.2967 - val_acc: 0.9070\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.2182 - acc: 0.9187 - val_loss: 0.2504 - val_acc: 0.9074\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1956 - acc: 0.9291 - val_loss: 0.2840 - val_acc: 0.8945\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1905 - acc: 0.9329 - val_loss: 0.2448 - val_acc: 0.9237\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1864 - acc: 0.9378 - val_loss: 0.2842 - val_acc: 0.8867\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.1598 - acc: 0.9408 - val_loss: 0.2925 - val_acc: 0.8877\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1653 - acc: 0.9357 - val_loss: 0.2872 - val_acc: 0.9036\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1703 - acc: 0.9381 - val_loss: 0.3389 - val_acc: 0.9026\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1610 - acc: 0.9436 - val_loss: 0.3357 - val_acc: 0.9033\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1429 - acc: 0.9484 - val_loss: 0.2664 - val_acc: 0.9138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1629 - acc: 0.9403 - val_loss: 0.3990 - val_acc: 0.8989\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.1372 - acc: 0.9446 - val_loss: 0.3375 - val_acc: 0.8968\n",
      "Test accuracy: 0.8968442483881914\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.3790 - acc: 0.4163 - val_loss: 1.5887 - val_acc: 0.3607\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 1.2000 - acc: 0.5128 - val_loss: 0.9426 - val_acc: 0.5609\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.9695 - acc: 0.5828 - val_loss: 0.9686 - val_acc: 0.4720\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.8395 - acc: 0.6304 - val_loss: 0.8240 - val_acc: 0.5982\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 116s 16ms/step - loss: 0.9826 - acc: 0.5751 - val_loss: 1.0264 - val_acc: 0.5612\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.8170 - acc: 0.6209 - val_loss: 0.7117 - val_acc: 0.6210\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.7924 - acc: 0.6364 - val_loss: 0.8239 - val_acc: 0.6271\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 115s 16ms/step - loss: 0.7206 - acc: 0.6681 - val_loss: 0.7129 - val_acc: 0.6841\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 115s 16ms/step - loss: 0.6576 - acc: 0.7138 - val_loss: 0.7840 - val_acc: 0.7574\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 114s 16ms/step - loss: 0.5931 - acc: 0.8020 - val_loss: 0.6623 - val_acc: 0.8052\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 114s 16ms/step - loss: 0.4535 - acc: 0.8697 - val_loss: 0.4164 - val_acc: 0.8531\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.3431 - acc: 0.8958 - val_loss: 0.3562 - val_acc: 0.8928\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 113s 15ms/step - loss: 0.3124 - acc: 0.9100 - val_loss: 0.3501 - val_acc: 0.9033\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 112s 15ms/step - loss: 0.2970 - acc: 0.9204 - val_loss: 0.4288 - val_acc: 0.8965\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 111s 15ms/step - loss: 0.3897 - acc: 0.8968 - val_loss: 0.5607 - val_acc: 0.8870\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 109s 15ms/step - loss: 0.2578 - acc: 0.9222 - val_loss: 0.5337 - val_acc: 0.9013\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 112s 15ms/step - loss: 0.2654 - acc: 0.9275 - val_loss: 0.8437 - val_acc: 0.8884\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 108s 15ms/step - loss: 0.2685 - acc: 0.9256 - val_loss: 0.5361 - val_acc: 0.9002\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 114s 16ms/step - loss: 0.2487 - acc: 0.9253 - val_loss: 0.6074 - val_acc: 0.8907\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 117s 16ms/step - loss: 0.3612 - acc: 0.8792 - val_loss: 0.5659 - val_acc: 0.8965\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 109s 15ms/step - loss: 0.2977 - acc: 0.9259 - val_loss: 0.3567 - val_acc: 0.9118\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.2833 - acc: 0.9244 - val_loss: 0.2639 - val_acc: 0.9145\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 116s 16ms/step - loss: 0.2543 - acc: 0.9248 - val_loss: 0.6417 - val_acc: 0.8901\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 115s 16ms/step - loss: 0.2711 - acc: 0.9314 - val_loss: 0.3078 - val_acc: 0.9155\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 113s 15ms/step - loss: 0.4324 - acc: 0.9197 - val_loss: 2.2120 - val_acc: 0.8045\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 112s 15ms/step - loss: 0.4195 - acc: 0.9158 - val_loss: 0.4296 - val_acc: 0.9101\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 108s 15ms/step - loss: 0.2178 - acc: 0.9314 - val_loss: 0.5475 - val_acc: 0.9006\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 113s 15ms/step - loss: 0.2168 - acc: 0.9355 - val_loss: 0.5504 - val_acc: 0.9141\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 109s 15ms/step - loss: 0.4097 - acc: 0.9183 - val_loss: 0.4351 - val_acc: 0.9128\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 112s 15ms/step - loss: 0.3899 - acc: 0.9180 - val_loss: 0.6097 - val_acc: 0.8755\n",
      "Test accuracy: 0.8754665761791652\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 1.3698 - acc: 0.4022 - val_loss: 1.1994 - val_acc: 0.4740\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 1.0903 - acc: 0.5367 - val_loss: 1.1028 - val_acc: 0.5389\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 1.1682 - acc: 0.4849 - val_loss: 1.1305 - val_acc: 0.4893\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.7807 - acc: 0.6500 - val_loss: 1.1183 - val_acc: 0.5382\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.5578 - acc: 0.7782 - val_loss: 0.6309 - val_acc: 0.7418\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.3260 - acc: 0.8810 - val_loss: 0.4220 - val_acc: 0.7560\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.2302 - acc: 0.9163 - val_loss: 0.3320 - val_acc: 0.8996\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1998 - acc: 0.9256 - val_loss: 0.2501 - val_acc: 0.9094\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.2116 - acc: 0.9255 - val_loss: 0.2443 - val_acc: 0.9080\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1678 - acc: 0.9316 - val_loss: 0.2699 - val_acc: 0.8962\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1552 - acc: 0.9416 - val_loss: 0.3336 - val_acc: 0.8738\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1482 - acc: 0.9463 - val_loss: 0.2744 - val_acc: 0.9152\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.1379 - acc: 0.9463 - val_loss: 0.2906 - val_acc: 0.9216\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.1436 - acc: 0.9459 - val_loss: 0.2730 - val_acc: 0.9091\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1345 - acc: 0.9487 - val_loss: 0.3552 - val_acc: 0.8863\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1323 - acc: 0.9489 - val_loss: 0.2880 - val_acc: 0.9250\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1275 - acc: 0.9490 - val_loss: 0.3459 - val_acc: 0.9141\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1359 - acc: 0.9472 - val_loss: 0.2172 - val_acc: 0.9257\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1243 - acc: 0.9508 - val_loss: 0.2902 - val_acc: 0.9199\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1297 - acc: 0.9489 - val_loss: 0.3606 - val_acc: 0.9070\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1319 - acc: 0.9487 - val_loss: 0.2641 - val_acc: 0.9220\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1300 - acc: 0.9494 - val_loss: 0.2310 - val_acc: 0.9206\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1212 - acc: 0.9504 - val_loss: 0.2364 - val_acc: 0.9267\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1228 - acc: 0.9543 - val_loss: 0.2441 - val_acc: 0.9237\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1157 - acc: 0.9540 - val_loss: 0.2284 - val_acc: 0.9260\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1283 - acc: 0.9528 - val_loss: 0.2717 - val_acc: 0.9084\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1192 - acc: 0.9518 - val_loss: 0.2112 - val_acc: 0.9237\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 91s 12ms/step - loss: 0.1225 - acc: 0.9489 - val_loss: 0.3002 - val_acc: 0.9182\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.1260 - acc: 0.9533 - val_loss: 0.2390 - val_acc: 0.9253\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 0.1142 - acc: 0.9544 - val_loss: 0.2432 - val_acc: 0.9359\n",
      "Test accuracy: 0.9358669833729216\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 1.3008 - acc: 0.4169 - val_loss: 1.1901 - val_acc: 0.5199\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 47s 6ms/step - loss: 1.1000 - acc: 0.4893 - val_loss: 1.1207 - val_acc: 0.4954\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 1.0713 - acc: 0.5245 - val_loss: 1.2818 - val_acc: 0.4992\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.7811 - acc: 0.6200 - val_loss: 1.4036 - val_acc: 0.4065\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.7401 - acc: 0.6364 - val_loss: 0.7953 - val_acc: 0.5962\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.9408 - acc: 0.5734 - val_loss: 1.2124 - val_acc: 0.4418\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.8508 - acc: 0.5819 - val_loss: 0.8116 - val_acc: 0.6271\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.6312 - acc: 0.7148 - val_loss: 0.7411 - val_acc: 0.6960\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.4257 - acc: 0.8410 - val_loss: 0.3409 - val_acc: 0.8785\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.2899 - acc: 0.9004 - val_loss: 0.5336 - val_acc: 0.8246\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.2299 - acc: 0.9244 - val_loss: 0.2797 - val_acc: 0.9053\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.1991 - acc: 0.9317 - val_loss: 0.3102 - val_acc: 0.9016\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 47s 6ms/step - loss: 0.1989 - acc: 0.9329 - val_loss: 0.3650 - val_acc: 0.8795\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1640 - acc: 0.9380 - val_loss: 0.2827 - val_acc: 0.9148\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.1503 - acc: 0.9416 - val_loss: 0.3698 - val_acc: 0.8996\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1704 - acc: 0.9407 - val_loss: 0.2457 - val_acc: 0.9155\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.1518 - acc: 0.9448 - val_loss: 0.3255 - val_acc: 0.8965\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.1529 - acc: 0.9453 - val_loss: 0.3203 - val_acc: 0.9152\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 47s 6ms/step - loss: 0.1468 - acc: 0.9474 - val_loss: 0.3206 - val_acc: 0.9121\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 47s 6ms/step - loss: 0.1444 - acc: 0.9478 - val_loss: 0.3120 - val_acc: 0.9138\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.1360 - acc: 0.9506 - val_loss: 0.2789 - val_acc: 0.9257\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.1347 - acc: 0.9505 - val_loss: 0.6842 - val_acc: 0.8565\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.1359 - acc: 0.9487 - val_loss: 0.3316 - val_acc: 0.9182\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 46s 6ms/step - loss: 0.1272 - acc: 0.9508 - val_loss: 0.3261 - val_acc: 0.9165\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 48s 7ms/step - loss: 0.1271 - acc: 0.9528 - val_loss: 0.2801 - val_acc: 0.9291\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 48s 6ms/step - loss: 0.1254 - acc: 0.9514 - val_loss: 0.2819 - val_acc: 0.9250\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 48s 7ms/step - loss: 0.1200 - acc: 0.9538 - val_loss: 0.4392 - val_acc: 0.9108\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 48s 7ms/step - loss: 0.1357 - acc: 0.9502 - val_loss: 0.3752 - val_acc: 0.9040\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 48s 6ms/step - loss: 0.1215 - acc: 0.9533 - val_loss: 0.3386 - val_acc: 0.9080\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 47s 6ms/step - loss: 0.1141 - acc: 0.9543 - val_loss: 0.3957 - val_acc: 0.9108\n",
      "Test accuracy: 0.9107567017305734\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 1.3154 - acc: 0.4287 - val_loss: 1.2448 - val_acc: 0.4645\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 1.1578 - acc: 0.5001 - val_loss: 0.8977 - val_acc: 0.5660\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.8386 - acc: 0.6571 - val_loss: 0.7195 - val_acc: 0.7241\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.4539 - acc: 0.8356 - val_loss: 0.3798 - val_acc: 0.8510\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.2905 - acc: 0.8900 - val_loss: 0.4832 - val_acc: 0.8358\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.2227 - acc: 0.9189 - val_loss: 0.3086 - val_acc: 0.8812\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.1950 - acc: 0.9270 - val_loss: 0.3672 - val_acc: 0.9074\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.1617 - acc: 0.9397 - val_loss: 0.3705 - val_acc: 0.8982\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.1613 - acc: 0.9408 - val_loss: 0.2987 - val_acc: 0.9040\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.1562 - acc: 0.9429 - val_loss: 0.5068 - val_acc: 0.8897\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1609 - acc: 0.9418 - val_loss: 0.4135 - val_acc: 0.8958\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.2207 - acc: 0.9057 - val_loss: 0.2987 - val_acc: 0.9023\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.1466 - acc: 0.9448 - val_loss: 0.3185 - val_acc: 0.9101\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1486 - acc: 0.9453 - val_loss: 0.3205 - val_acc: 0.9121\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.1390 - acc: 0.9483 - val_loss: 0.4126 - val_acc: 0.9131\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1435 - acc: 0.9444 - val_loss: 0.3551 - val_acc: 0.9233\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1420 - acc: 0.9474 - val_loss: 0.4149 - val_acc: 0.8456\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.1351 - acc: 0.9468 - val_loss: 0.3911 - val_acc: 0.9182\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1609 - acc: 0.9474 - val_loss: 0.4582 - val_acc: 0.9148\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1320 - acc: 0.9491 - val_loss: 0.3044 - val_acc: 0.9301\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.1231 - acc: 0.9523 - val_loss: 0.4043 - val_acc: 0.9141\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.1437 - acc: 0.9474 - val_loss: 0.4021 - val_acc: 0.9169\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.1451 - acc: 0.9490 - val_loss: 0.4310 - val_acc: 0.9155\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1328 - acc: 0.9471 - val_loss: 0.2989 - val_acc: 0.9233\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 129s 17ms/step - loss: 0.1450 - acc: 0.9468 - val_loss: 0.4329 - val_acc: 0.9148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1401 - acc: 0.9490 - val_loss: 0.3741 - val_acc: 0.9230\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1402 - acc: 0.9489 - val_loss: 0.3374 - val_acc: 0.9267\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1493 - acc: 0.9509 - val_loss: 0.2570 - val_acc: 0.9186\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1295 - acc: 0.9517 - val_loss: 0.4193 - val_acc: 0.9158\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1539 - acc: 0.9423 - val_loss: 0.4205 - val_acc: 0.9067\n",
      "Test accuracy: 0.9066847641871749\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 73s 10ms/step - loss: 1.4068 - acc: 0.3814 - val_loss: 1.3328 - val_acc: 0.4404\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 1.2975 - acc: 0.4369 - val_loss: 1.2833 - val_acc: 0.4299\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 1.2589 - acc: 0.4236 - val_loss: 1.2551 - val_acc: 0.4591\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 70s 9ms/step - loss: 0.8914 - acc: 0.5939 - val_loss: 0.8661 - val_acc: 0.6071\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.8391 - acc: 0.6300 - val_loss: 0.8848 - val_acc: 0.5870\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.9193 - acc: 0.5832 - val_loss: 1.9246 - val_acc: 0.4116\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.9039 - acc: 0.6088 - val_loss: 0.9329 - val_acc: 0.5623\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.7931 - acc: 0.6465 - val_loss: 0.6820 - val_acc: 0.6997\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.6964 - acc: 0.7054 - val_loss: 0.7448 - val_acc: 0.6905\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.5899 - acc: 0.7586 - val_loss: 0.5581 - val_acc: 0.7720\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.5426 - acc: 0.8022 - val_loss: 0.4151 - val_acc: 0.8534\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.4032 - acc: 0.8587 - val_loss: 0.7167 - val_acc: 0.7978\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.4117 - acc: 0.8555 - val_loss: 0.3870 - val_acc: 0.8721\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.2552 - acc: 0.9095 - val_loss: 0.4325 - val_acc: 0.8609\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.2443 - acc: 0.9157 - val_loss: 0.4188 - val_acc: 0.8758\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.2229 - acc: 0.9203 - val_loss: 0.4483 - val_acc: 0.8785\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1976 - acc: 0.9261 - val_loss: 0.5582 - val_acc: 0.8273\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1966 - acc: 0.9334 - val_loss: 0.4417 - val_acc: 0.8931\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.2038 - acc: 0.9236 - val_loss: 0.3587 - val_acc: 0.8935\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1831 - acc: 0.9325 - val_loss: 1.1522 - val_acc: 0.7978\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1763 - acc: 0.9368 - val_loss: 0.5223 - val_acc: 0.8755\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1853 - acc: 0.9325 - val_loss: 0.4337 - val_acc: 0.9013\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.1629 - acc: 0.9380 - val_loss: 0.5268 - val_acc: 0.8931\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1641 - acc: 0.9436 - val_loss: 0.4764 - val_acc: 0.8870\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1486 - acc: 0.9441 - val_loss: 0.4453 - val_acc: 0.8748\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1771 - acc: 0.9416 - val_loss: 0.9084 - val_acc: 0.8191\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1483 - acc: 0.9412 - val_loss: 0.4677 - val_acc: 0.8982\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1633 - acc: 0.9440 - val_loss: 0.4241 - val_acc: 0.8992\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1317 - acc: 0.9474 - val_loss: 0.3439 - val_acc: 0.9060\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 70s 9ms/step - loss: 0.1461 - acc: 0.9474 - val_loss: 0.4540 - val_acc: 0.8992\n",
      "Test accuracy: 0.8992195453003053\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 1.3240 - acc: 0.4343 - val_loss: 1.2227 - val_acc: 0.4785\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 1.0488 - acc: 0.5567 - val_loss: 0.9246 - val_acc: 0.5819\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.8679 - acc: 0.6364 - val_loss: 0.8721 - val_acc: 0.6305\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.7630 - acc: 0.6798 - val_loss: 0.8584 - val_acc: 0.6620\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.7343 - acc: 0.7081 - val_loss: 0.7365 - val_acc: 0.6820\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.6541 - acc: 0.7661 - val_loss: 0.7389 - val_acc: 0.7659\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.5313 - acc: 0.8207 - val_loss: 0.4731 - val_acc: 0.8476\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 59s 8ms/step - loss: 0.4648 - acc: 0.8526 - val_loss: 0.4400 - val_acc: 0.8378\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.4244 - acc: 0.8834 - val_loss: 0.5648 - val_acc: 0.8419\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.3312 - acc: 0.9014 - val_loss: 0.4692 - val_acc: 0.8585\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.3012 - acc: 0.9078 - val_loss: 0.3977 - val_acc: 0.8734\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.3310 - acc: 0.9110 - val_loss: 0.3563 - val_acc: 0.8860\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2860 - acc: 0.9131 - val_loss: 0.4326 - val_acc: 0.8809\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2892 - acc: 0.9184 - val_loss: 0.3172 - val_acc: 0.8948\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2535 - acc: 0.9202 - val_loss: 0.4036 - val_acc: 0.8999\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2573 - acc: 0.9227 - val_loss: 0.4764 - val_acc: 0.8755\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2453 - acc: 0.9287 - val_loss: 0.6415 - val_acc: 0.8863\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 60s 8ms/step - loss: 0.2532 - acc: 0.9217 - val_loss: 0.3521 - val_acc: 0.8897\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2057 - acc: 0.9368 - val_loss: 0.3870 - val_acc: 0.8707\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2082 - acc: 0.9332 - val_loss: 0.3583 - val_acc: 0.9108\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2115 - acc: 0.9338 - val_loss: 0.7571 - val_acc: 0.8782\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.1892 - acc: 0.9314 - val_loss: 0.5819 - val_acc: 0.8870\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2053 - acc: 0.9347 - val_loss: 0.4435 - val_acc: 0.9121\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2181 - acc: 0.9350 - val_loss: 0.4340 - val_acc: 0.9118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2229 - acc: 0.9362 - val_loss: 0.3305 - val_acc: 0.9101\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2008 - acc: 0.9408 - val_loss: 0.4580 - val_acc: 0.8955\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2664 - acc: 0.9319 - val_loss: 0.6328 - val_acc: 0.8992\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.2243 - acc: 0.9397 - val_loss: 0.8507 - val_acc: 0.8714\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2212 - acc: 0.9363 - val_loss: 0.3582 - val_acc: 0.9111\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 60s 8ms/step - loss: 0.1751 - acc: 0.9430 - val_loss: 0.4272 - val_acc: 0.9019\n",
      "Test accuracy: 0.9019341703427214\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 1.3341 - acc: 0.4143 - val_loss: 1.3362 - val_acc: 0.3570\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 1.2000 - acc: 0.4474 - val_loss: 1.3525 - val_acc: 0.3760\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 1.1536 - acc: 0.4755 - val_loss: 0.9473 - val_acc: 0.6020\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.9045 - acc: 0.5871 - val_loss: 0.8491 - val_acc: 0.6003\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.7889 - acc: 0.6518 - val_loss: 0.7489 - val_acc: 0.6549\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.7957 - acc: 0.6654 - val_loss: 0.7173 - val_acc: 0.7180\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.5563 - acc: 0.7905 - val_loss: 0.5978 - val_acc: 0.7638\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.4408 - acc: 0.8439 - val_loss: 0.4464 - val_acc: 0.8371\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.3431 - acc: 0.8787 - val_loss: 0.3729 - val_acc: 0.8666\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2753 - acc: 0.9041 - val_loss: 0.4682 - val_acc: 0.8205\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2191 - acc: 0.9196 - val_loss: 0.4380 - val_acc: 0.8663\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2124 - acc: 0.9196 - val_loss: 0.3059 - val_acc: 0.8941\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1865 - acc: 0.9283 - val_loss: 0.2709 - val_acc: 0.9016\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2055 - acc: 0.9279 - val_loss: 0.2660 - val_acc: 0.8951\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1726 - acc: 0.9306 - val_loss: 0.2695 - val_acc: 0.9067\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1609 - acc: 0.9399 - val_loss: 0.2304 - val_acc: 0.9074\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2322 - acc: 0.9316 - val_loss: 0.2630 - val_acc: 0.9040\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1574 - acc: 0.9406 - val_loss: 0.2288 - val_acc: 0.9135\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1749 - acc: 0.9388 - val_loss: 0.2646 - val_acc: 0.8968\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1546 - acc: 0.9358 - val_loss: 0.2586 - val_acc: 0.9108\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1505 - acc: 0.9421 - val_loss: 0.2522 - val_acc: 0.8951\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1414 - acc: 0.9433 - val_loss: 0.2839 - val_acc: 0.9050\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1401 - acc: 0.9444 - val_loss: 0.2427 - val_acc: 0.9067\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1264 - acc: 0.9499 - val_loss: 0.3614 - val_acc: 0.8951\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1226 - acc: 0.9499 - val_loss: 0.3367 - val_acc: 0.8996\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1355 - acc: 0.9490 - val_loss: 0.2775 - val_acc: 0.9084\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1266 - acc: 0.9513 - val_loss: 0.2273 - val_acc: 0.9209\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1250 - acc: 0.9504 - val_loss: 0.2206 - val_acc: 0.9328\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1257 - acc: 0.9470 - val_loss: 0.2694 - val_acc: 0.9206\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1226 - acc: 0.9487 - val_loss: 0.2492 - val_acc: 0.9213\n",
      "Test accuracy: 0.9212758737699356\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 1.4584 - acc: 0.3760 - val_loss: 1.4380 - val_acc: 0.3990\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 1.1045 - acc: 0.5184 - val_loss: 0.8487 - val_acc: 0.6237\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.9701 - acc: 0.5918 - val_loss: 0.7451 - val_acc: 0.6759\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.6769 - acc: 0.7251 - val_loss: 0.6333 - val_acc: 0.7492\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.4460 - acc: 0.8473 - val_loss: 0.3466 - val_acc: 0.8700\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.3325 - acc: 0.8965 - val_loss: 1.6093 - val_acc: 0.6888\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.2486 - acc: 0.9157 - val_loss: 0.3690 - val_acc: 0.8975\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.2108 - acc: 0.9260 - val_loss: 0.2507 - val_acc: 0.9145\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1831 - acc: 0.9358 - val_loss: 0.2937 - val_acc: 0.9145\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1855 - acc: 0.9335 - val_loss: 0.2236 - val_acc: 0.9172\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1729 - acc: 0.9362 - val_loss: 0.3270 - val_acc: 0.9033\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 131s 18ms/step - loss: 0.1939 - acc: 0.9384 - val_loss: 0.3628 - val_acc: 0.8914\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 131s 18ms/step - loss: 0.1650 - acc: 0.9425 - val_loss: 0.2515 - val_acc: 0.9196\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1612 - acc: 0.9441 - val_loss: 0.2220 - val_acc: 0.9226\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1473 - acc: 0.9455 - val_loss: 0.2666 - val_acc: 0.9209\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1552 - acc: 0.9422 - val_loss: 0.2500 - val_acc: 0.9043\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1641 - acc: 0.9404 - val_loss: 0.4475 - val_acc: 0.9182\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1610 - acc: 0.9453 - val_loss: 0.4994 - val_acc: 0.8636\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1530 - acc: 0.9465 - val_loss: 0.3641 - val_acc: 0.9101\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1390 - acc: 0.9464 - val_loss: 0.2430 - val_acc: 0.9291\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1493 - acc: 0.9478 - val_loss: 0.3947 - val_acc: 0.9067\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1491 - acc: 0.9446 - val_loss: 0.3425 - val_acc: 0.9128\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1735 - acc: 0.9418 - val_loss: 0.3824 - val_acc: 0.9063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.1471 - acc: 0.9489 - val_loss: 0.2361 - val_acc: 0.9243\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.1464 - acc: 0.9445 - val_loss: 0.4671 - val_acc: 0.9080\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.1644 - acc: 0.9442 - val_loss: 0.6430 - val_acc: 0.8955\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1412 - acc: 0.9467 - val_loss: 0.2782 - val_acc: 0.9399\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.1347 - acc: 0.9499 - val_loss: 0.4034 - val_acc: 0.9087\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.1309 - acc: 0.9510 - val_loss: 0.3121 - val_acc: 0.9091\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1410 - acc: 0.9483 - val_loss: 0.2925 - val_acc: 0.9294\n",
      "Test accuracy: 0.9294197488971836\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 1.2342 - acc: 0.4706 - val_loss: 1.3749 - val_acc: 0.4316\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.8904 - acc: 0.5949 - val_loss: 0.8673 - val_acc: 0.5640\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.8502 - acc: 0.6265 - val_loss: 0.9527 - val_acc: 0.6159\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.6851 - acc: 0.7223 - val_loss: 0.8160 - val_acc: 0.6542\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.5706 - acc: 0.7654 - val_loss: 0.5959 - val_acc: 0.7628\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.4837 - acc: 0.8154 - val_loss: 0.4920 - val_acc: 0.8351\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.4048 - acc: 0.8575 - val_loss: 0.5652 - val_acc: 0.8018\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2959 - acc: 0.8996 - val_loss: 0.4047 - val_acc: 0.8741\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.2485 - acc: 0.9131 - val_loss: 0.4794 - val_acc: 0.8649\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2406 - acc: 0.9174 - val_loss: 0.3802 - val_acc: 0.8541\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1973 - acc: 0.9289 - val_loss: 0.3074 - val_acc: 0.8985\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1830 - acc: 0.9347 - val_loss: 0.3364 - val_acc: 0.8982\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1681 - acc: 0.9376 - val_loss: 0.2768 - val_acc: 0.9091\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1654 - acc: 0.9408 - val_loss: 0.2903 - val_acc: 0.9067\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1532 - acc: 0.9387 - val_loss: 0.2796 - val_acc: 0.9030\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1544 - acc: 0.9436 - val_loss: 0.2680 - val_acc: 0.9186\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1497 - acc: 0.9441 - val_loss: 0.2810 - val_acc: 0.9148\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1412 - acc: 0.9478 - val_loss: 0.2536 - val_acc: 0.9138\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1385 - acc: 0.9497 - val_loss: 0.2782 - val_acc: 0.9189\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1388 - acc: 0.9499 - val_loss: 0.2757 - val_acc: 0.9138\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1381 - acc: 0.9482 - val_loss: 0.2572 - val_acc: 0.9084\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1376 - acc: 0.9489 - val_loss: 0.3220 - val_acc: 0.9070\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1259 - acc: 0.9484 - val_loss: 0.3722 - val_acc: 0.9104\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1350 - acc: 0.9505 - val_loss: 0.2632 - val_acc: 0.9138\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1262 - acc: 0.9509 - val_loss: 0.3359 - val_acc: 0.9084\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1389 - acc: 0.9479 - val_loss: 0.2769 - val_acc: 0.9114\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1261 - acc: 0.9533 - val_loss: 0.3538 - val_acc: 0.9033\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1248 - acc: 0.9513 - val_loss: 0.3200 - val_acc: 0.9036\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1295 - acc: 0.9494 - val_loss: 0.3320 - val_acc: 0.9158\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1235 - acc: 0.9517 - val_loss: 0.3404 - val_acc: 0.9050\n",
      "Test accuracy: 0.9049881235154394\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=load_data,\n",
    "                                          max_evals=10,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          notebook_name='HAR_LSTM',\n",
    "                                          trials=trials\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDlxZBLbjU4G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "2947/2947 [==============================] - 8s 3ms/step\n",
      "[0.24316548304311456, 0.9358669833729216]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'LSTM': 2, 'Dropout': 0.288662535902546, 'batch_size': 1}\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 96s 13ms/step - loss: 1.3417 - acc: 0.4068 - val_loss: 1.1853 - val_acc: 0.5243\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 1.1744 - acc: 0.4770 - val_loss: 1.1168 - val_acc: 0.4700\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 1.0587 - acc: 0.5163 - val_loss: 0.9571 - val_acc: 0.5222\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 0.7292 - acc: 0.6752 - val_loss: 0.9916 - val_acc: 0.5996\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.6078 - acc: 0.7599 - val_loss: 0.5368 - val_acc: 0.7767\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.4263 - acc: 0.8441 - val_loss: 0.3994 - val_acc: 0.8656\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.2956 - acc: 0.8993 - val_loss: 0.2945 - val_acc: 0.8907\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.2548 - acc: 0.9075 - val_loss: 0.3646 - val_acc: 0.8816\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.2054 - acc: 0.9255 - val_loss: 0.3065 - val_acc: 0.8873\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.1790 - acc: 0.9331 - val_loss: 0.3470 - val_acc: 0.8958\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.1700 - acc: 0.9378 - val_loss: 0.3698 - val_acc: 0.8870\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 0.1709 - acc: 0.9336 - val_loss: 0.4295 - val_acc: 0.8901\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.1510 - acc: 0.9427 - val_loss: 0.3111 - val_acc: 0.9111\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.1611 - acc: 0.9426 - val_loss: 0.2664 - val_acc: 0.9084\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.1426 - acc: 0.9480 - val_loss: 0.3880 - val_acc: 0.8941\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.1254 - acc: 0.9517 - val_loss: 0.4313 - val_acc: 0.8955\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.1490 - acc: 0.9419 - val_loss: 0.4394 - val_acc: 0.8992\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.1238 - acc: 0.9494 - val_loss: 0.5041 - val_acc: 0.8968\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.1294 - acc: 0.9508 - val_loss: 0.3452 - val_acc: 0.9094\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.1301 - acc: 0.9489 - val_loss: 0.3721 - val_acc: 0.9060\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.1280 - acc: 0.9495 - val_loss: 0.2238 - val_acc: 0.9304\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.1261 - acc: 0.9491 - val_loss: 0.3413 - val_acc: 0.9135\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 91s 12ms/step - loss: 0.1283 - acc: 0.9501 - val_loss: 0.2854 - val_acc: 0.9108\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.1196 - acc: 0.9533 - val_loss: 0.3144 - val_acc: 0.9114\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 0.1156 - acc: 0.9529 - val_loss: 0.2865 - val_acc: 0.9111\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 0.1294 - acc: 0.9517 - val_loss: 0.2388 - val_acc: 0.9186\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 91s 12ms/step - loss: 0.1226 - acc: 0.9509 - val_loss: 0.2108 - val_acc: 0.9213\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.1196 - acc: 0.9536 - val_loss: 0.3917 - val_acc: 0.9060\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 0.1198 - acc: 0.9531 - val_loss: 0.3241 - val_acc: 0.9094\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 91s 12ms/step - loss: 0.1189 - acc: 0.9524 - val_loss: 0.2353 - val_acc: 0.9260\n",
      "Test accuracy: 0.9260264675941635\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(256, input_shape=(128, 9)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.288662535902546))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='rmsprop',\n",
    "          metrics=['accuracy'])\n",
    "# Training the model\n",
    "history = model.fit(X_train,\n",
    "        Y_train,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        epochs=30)\n",
    "score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
    "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "# list of epoch numbers\n",
    "x = list(range(1,31))\n",
    "\n",
    "\n",
    "vy = history.history['val_loss']\n",
    "ty = history.history['loss']\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyperas is used for hyperparmater tuning.\n",
    "2. Number of Epoch =30 \n",
    "3. model() function contains the model for hyperparameter tuning.\n",
    "4. load_data() is used for loading the data.\n",
    "5. for LSTM I used values 64, 128, 256\n",
    "6. for Dropouts , I take value from uniform distribution of 0 and 1 and apply on each model\n",
    "7. for batch size , I used values 16,32,64\n",
    "8. After hyperparameter tuning, I got no of LSTM-256,0.288662535902546-dropout rate, 32-batch size as the best values.\n",
    "9. With these values, I train the model and got the test accuracy of 92.6%\n",
    "10. Train and validaton loss is also plotted for the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| LSTM | Dropout | Batch size | Accuracy |\n",
    "|------|---------|------------|----------|\n",
    "| 256  | 0.29    | 32         | 92.6     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HAR_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
